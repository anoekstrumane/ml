{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:#00868b'>Read, balance and clean dataset<span class=\"tocSkip\"></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"corpus_sprint3_balanced_cleaned_all.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set index to old index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Row No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126593, 18)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date received</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub-product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Sub-issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Company public response</th>\n",
       "      <th>Company</th>\n",
       "      <th>State</th>\n",
       "      <th>ZIP code</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Consumer consent provided?</th>\n",
       "      <th>Submitted via</th>\n",
       "      <th>Date sent to company</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Timely response?</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "      <th>Complaint ID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Row No</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>06/15/19</td>\n",
       "      <td>Payday loan, title loan, or personal loan</td>\n",
       "      <td>Installment loan</td>\n",
       "      <td>Problem with the payoff process at the end of ...</td>\n",
       "      <td>None</td>\n",
       "      <td>they would not let me pay my loan off days bef...</td>\n",
       "      <td>None</td>\n",
       "      <td>Big Picture Loans, LLC</td>\n",
       "      <td>IN</td>\n",
       "      <td>477XX</td>\n",
       "      <td>None</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>06/15/19</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3276316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>07/25/19</td>\n",
       "      <td>Payday loan, title loan, or personal loan</td>\n",
       "      <td>Installment loan</td>\n",
       "      <td>Charged fees or interest you didn't expect</td>\n",
       "      <td>None</td>\n",
       "      <td>service finance are liars and are charging me ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Service Finance Holdings, LLC</td>\n",
       "      <td>TX</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>07/25/19</td>\n",
       "      <td>Closed with non-monetary relief</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3318533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>06/23/19</td>\n",
       "      <td>Vehicle loan or lease</td>\n",
       "      <td>Loan</td>\n",
       "      <td>Problems at the end of the loan or lease</td>\n",
       "      <td>Problem with paying off the loan</td>\n",
       "      <td>on i signed a car loan agreement to finance my...</td>\n",
       "      <td>None</td>\n",
       "      <td>HUNTINGTON NATIONAL BANK, THE</td>\n",
       "      <td>TX</td>\n",
       "      <td>750XX</td>\n",
       "      <td>None</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>06/23/19</td>\n",
       "      <td>Closed with non-monetary relief</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3284279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>08/13/19</td>\n",
       "      <td>Money transfer, virtual currency, or money ser...</td>\n",
       "      <td>Debt settlement</td>\n",
       "      <td>Fraud or scam</td>\n",
       "      <td>None</td>\n",
       "      <td>we hired and debt collection to handle collect...</td>\n",
       "      <td>None</td>\n",
       "      <td>ALLIED NATIONAL INC</td>\n",
       "      <td>NY</td>\n",
       "      <td>117XX</td>\n",
       "      <td>None</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>08/13/19</td>\n",
       "      <td>Untimely response</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3339246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>07/31/19</td>\n",
       "      <td>Payday loan, title loan, or personal loan</td>\n",
       "      <td>Payday loan</td>\n",
       "      <td>Problem with the payoff process at the end of ...</td>\n",
       "      <td>None</td>\n",
       "      <td>i borrowed in an financial emergency from offi...</td>\n",
       "      <td>Company believes it acted appropriately as aut...</td>\n",
       "      <td>Harpeth Financial Services, LLC</td>\n",
       "      <td>TN</td>\n",
       "      <td>None</td>\n",
       "      <td>Servicemember</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>08/02/19</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3324772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date received                                            Product  \\\n",
       "Row No                                                                    \n",
       "9           06/15/19          Payday loan, title loan, or personal loan   \n",
       "12          07/25/19          Payday loan, title loan, or personal loan   \n",
       "38          06/23/19                              Vehicle loan or lease   \n",
       "44          08/13/19  Money transfer, virtual currency, or money ser...   \n",
       "52          07/31/19          Payday loan, title loan, or personal loan   \n",
       "\n",
       "             Sub-product                                              Issue  \\\n",
       "Row No                                                                        \n",
       "9       Installment loan  Problem with the payoff process at the end of ...   \n",
       "12      Installment loan         Charged fees or interest you didn't expect   \n",
       "38                  Loan           Problems at the end of the loan or lease   \n",
       "44       Debt settlement                                      Fraud or scam   \n",
       "52           Payday loan  Problem with the payoff process at the end of ...   \n",
       "\n",
       "                               Sub-issue  \\\n",
       "Row No                                     \n",
       "9                                   None   \n",
       "12                                  None   \n",
       "38      Problem with paying off the loan   \n",
       "44                                  None   \n",
       "52                                  None   \n",
       "\n",
       "                             Consumer complaint narrative  \\\n",
       "Row No                                                      \n",
       "9       they would not let me pay my loan off days bef...   \n",
       "12      service finance are liars and are charging me ...   \n",
       "38      on i signed a car loan agreement to finance my...   \n",
       "44      we hired and debt collection to handle collect...   \n",
       "52      i borrowed in an financial emergency from offi...   \n",
       "\n",
       "                                  Company public response  \\\n",
       "Row No                                                      \n",
       "9                                                    None   \n",
       "12                                                   None   \n",
       "38                                                   None   \n",
       "44                                                   None   \n",
       "52      Company believes it acted appropriately as aut...   \n",
       "\n",
       "                                Company State ZIP code           Tags  \\\n",
       "Row No                                                                  \n",
       "9                Big Picture Loans, LLC    IN    477XX           None   \n",
       "12        Service Finance Holdings, LLC    TX     None           None   \n",
       "38        HUNTINGTON NATIONAL BANK, THE    TX    750XX           None   \n",
       "44                  ALLIED NATIONAL INC    NY    117XX           None   \n",
       "52      Harpeth Financial Services, LLC    TN     None  Servicemember   \n",
       "\n",
       "       Consumer consent provided? Submitted via Date sent to company  \\\n",
       "Row No                                                                 \n",
       "9                Consent provided           Web             06/15/19   \n",
       "12               Consent provided           Web             07/25/19   \n",
       "38               Consent provided           Web             06/23/19   \n",
       "44               Consent provided           Web             08/13/19   \n",
       "52               Consent provided           Web             08/02/19   \n",
       "\n",
       "           Company response to consumer Timely response? Consumer disputed?  \\\n",
       "Row No                                                                        \n",
       "9               Closed with explanation              Yes                NaN   \n",
       "12      Closed with non-monetary relief              Yes                NaN   \n",
       "38      Closed with non-monetary relief              Yes                NaN   \n",
       "44                    Untimely response               No                NaN   \n",
       "52              Closed with explanation              Yes                NaN   \n",
       "\n",
       "        Complaint ID  \n",
       "Row No                \n",
       "9            3276316  \n",
       "12           3318533  \n",
       "38           3284279  \n",
       "44           3339246  \n",
       "52           3324772  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i borrowed in an financial emergency from office in tn on then on at in tn i was told my payoff is so i paid however i have been charged much more than on i paid off the loan in person and my checking account was also double billed charged another in tn today i went into the office in tn and asked for the reimbursement of my overbilled monies and the clerk named refused to refund my over double billed charges! threatened to have me arrested if i did not let the office keep the monies my loan of was just for days i was forced to pay a total of i was overbilled double billed '"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Consumer complaint narrative\"][52]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words and stem words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming reduces the variation in text data by converting words to their word stem. Applying stemming allows for LDA to focus much more finely on the base form of a word, rather than focusing on the differences in the various variations of a word. The code below is analagous to sprint 1's code in [<code>lemmatization.py</code>](lemmatization.py).\n",
    "\n",
    "Stop words include: \n",
    "* common English words;\n",
    "* state names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of extra list:  101\n",
      "Length of extra list:  143\n",
      "Length of my_stop_words list:  322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# use nltk.download('stopwords') to download the list of stop words if this is your first time using nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "my_additional_stop_words = []\n",
    "\n",
    "# US states, capitalised and lower\n",
    "states_abbr = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
    "          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
    "          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
    "          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n",
    "states = [\"Alabama\",\"Alaska\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\n",
    "  \"Connecticut\",\"Delaware\",\"Florida\",\"Georgia\",\"Hawaii\",\"Idaho\",\"Illinois\",\n",
    "  \"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\"Maryland\",\n",
    "  \"Massachusetts\",\"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\n",
    "  \"Nebraska\",\"Nevada\",\"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\n",
    "  \"North Carolina\",\"North Dakota\",\"Ohio\",\"Oklahoma\",\"Oregon\",\"Pennsylvania\",\n",
    "  \"Rhode Island\",\"South Carolina\",\"South Dakota\",\"Tennessee\",\"Texas\",\"Utah\",\n",
    "  \"Vermont\",\"Virginia\",\"Washington\",\"West Virginia\",\"Wisconsin\",\"Wyoming\"]\n",
    "\n",
    "states_abbr = [item.lower() for item in states_abbr]\n",
    "states = [item.lower() for item in states]\n",
    "states.extend(states_abbr)\n",
    "\n",
    "# add to list of additional stop words\n",
    "my_additional_stop_words.extend(states)\n",
    "print(\"Length of extra list: \", len(my_additional_stop_words))\n",
    "\n",
    "# add other stop words\n",
    "my_additional_stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "print(\"Length of extra list: \", len(my_additional_stop_words))\n",
    "\n",
    "# add nltk's stop words\n",
    "my_stop_words = stop_words + my_additional_stop_words\n",
    "print(\"Length of my_stop_words list: \", len(my_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bryan\\.conda\\envs\\ml\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(126593, 18)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "i = 0\n",
    "\n",
    "for ind, complaint in df[\"Consumer complaint narrative\"].items():\n",
    "    i = i+1\n",
    "    # Tokenize the complaint and remove stop words\n",
    "    words = [word for word in complaint.split(' ')\n",
    "                            if word not in my_stop_words] \n",
    "    new_words = []\n",
    "    # Stem the words in the complaint\n",
    "    for word in words:\n",
    "        new_words.append(ps.stem(word))\n",
    "    df[\"Consumer complaint narrative\"][ind] = new_words\n",
    "    if (i % 1000) == 0:\n",
    "        print(i)\n",
    "\n",
    "# to csv for later use\n",
    "df.to_csv(\"corpus_cleaned_and_stemmed_for_LDA.csv\", index=False)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['borrow',\n",
       " 'financ',\n",
       " 'emerg',\n",
       " 'offic',\n",
       " 'told',\n",
       " 'payoff',\n",
       " 'paid',\n",
       " 'howev',\n",
       " 'charg',\n",
       " 'much',\n",
       " 'paid',\n",
       " 'loan',\n",
       " 'person',\n",
       " 'check',\n",
       " 'account',\n",
       " 'doubl',\n",
       " 'bill',\n",
       " 'charg',\n",
       " 'anoth',\n",
       " 'today',\n",
       " 'went',\n",
       " 'offic',\n",
       " 'ask',\n",
       " 'reimbur',\n",
       " 'overbil',\n",
       " 'moni',\n",
       " 'clerk',\n",
       " 'name',\n",
       " 'refu',\n",
       " 'refund',\n",
       " 'doubl',\n",
       " 'bill',\n",
       " 'charges!',\n",
       " 'threaten',\n",
       " 'arrest',\n",
       " 'let',\n",
       " 'offic',\n",
       " 'keep',\n",
       " 'moni',\n",
       " 'loan',\n",
       " 'day',\n",
       " 'forc',\n",
       " 'pay',\n",
       " 'total',\n",
       " 'overbil',\n",
       " 'doubl',\n",
       " 'bill',\n",
       " '']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Consumer complaint narrative'][52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row No\n",
       "9         [let, pay, loan, day, next, payment, plu, subs...\n",
       "12        [servic, financ, liar, charg, interest, hvac, ...\n",
       "38        [sign, car, loan, agreement, financ, car, loan...\n",
       "44        [hire, debt, collect, handl, collect, effort, ...\n",
       "52        [borrow, financ, emerg, offic, told, payoff, p...\n",
       "                                ...                        \n",
       "311078    [bank, sever, year, bank, trust, origin, husba...\n",
       "368588    [file, fraud, alert, bank, week, ago, call, ev...\n",
       "256815    [previou, trip, bank, america, atm, card, cash...\n",
       "193394    [account, td, bank, check, save, start, work, ...\n",
       "257330    [hello, receiv, promot, offer, code, citibank,...\n",
       "Name: Consumer complaint narrative, Length: 126593, dtype: object"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now turned every complaint into an array of its words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenzation & Document Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two main inputs to our LDA topic model are the Dictionary (id2word) and the corpus. Let's create them. [source](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)\n",
    "\n",
    "We remove rare words and common words based on their document frequency. Below we remove words that appear in less than 20 documents or in more than 50% of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "id2word = Dictionary(df['Consumer complaint narrative'])\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "id2word.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we transform the documents to a vectorized form. We simply compute the frequency of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [id2word.doc2bow(doc) for doc in df['Consumer complaint narrative']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see how many tokens and documents we have to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 6378\n",
      "Number of documents: 126593\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(id2word))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the 126593 complaints is now represented as a 6378-dimensional vector, which means our vocabulary has 6378 words. The frequencies stated above (20 and 50% can be finetuned)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to apply LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters ([source](https://radimrehurek.com/gensim/models/ldamulticore.html)):\n",
    "* α: Topic smoothing parameter; can be set to an 1D array of length equal to the number of expected topics that expresses our a-priori belief for the each topics’ probability\n",
    "* eta: Word/term smoothing parameter; a scalar for a symmetric prior over topic/word probability\n",
    "\n",
    "Most topic modeling analyses in the literature ([Blei et al, 2003](https://www.researchgate.net/publication/326505884_Latent_Dirichlet_Allocation_LDA_for_Topic_Modeling_of_the_CFPB_Consumer_Complaints); [Blei and Lafferty, 2009](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.463.1205&rep=rep1&type=pdf#page=96); [Kaplan and Vakili, 2015](https://onlinelibrary.wiley.com/doi/abs/10.1002/smj.2294); [Blei, 2012](https://dl.acm.org/doi/pdf/10.1145/2133806.2133826)) suggest a value of 0.1 for both of these hyperparameters. This results in semantically meaningful topics. However, these values can also be set to 'auto', meaning we would automatically learn these two parameters.\n",
    "\n",
    "* number of topics: The number of topics LDA has to attempt to identify\n",
    "* iterations: Maximum number of iterations through the corpus when inferring the topic distribution of a corpus\n",
    "* passes: Number of passes through the corpus during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Rehurek, 2019 [source](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html):\n",
    "\n",
    "\"First of all, the elephant in the room: how many topics do I need? There is really no easy answer for this, it will depend on both your data and your application. I have used 10 topics here because I wanted to have a few topics that I could interpret and “label”, and because that turned out to give me reasonably good results. You might not need to interpret all your topics, so you could use a large number of topics, for example 100.\n",
    "\n",
    "<code>chunksize</code> controls how many documents are processed at a time in the training algorithm. Increasing chunksize will speed up training, at least as long as the chunk of documents easily fit into memory. Chunksize can however influence the quality of the model, as discussed in Hoffman and co-authors [source](https://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf).\n",
    "\n",
    "<code>passes</code> controls how often we train the model on the entire corpus. Another word for passes might be “epochs”. <code>iterations</code> is somewhat technical, but essentially it controls how often we repeat a particular loop over each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used **parallelized Latent Dirichlet Allocation** which uses multiprocessing to speed up learning ([source](https://radimrehurek.com/gensim/models/ldamulticore.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start with number of topics: 5\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "from sklearn import metrics\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "\n",
    "# Training parameters\n",
    "alpha = 0.1\n",
    "eta = 0.1\n",
    "#alpha = 'auto'\n",
    "#eta = 'auto'\n",
    "num_topics = [5,10,15,20] # sizes of the topics\n",
    "chunksize = 2000\n",
    "iterations = 400\n",
    "passes = 20\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "for topics in num_topics:\n",
    "    print(\"Start with number of topics:\", topics)\n",
    "    lda_model = models.LdaMulticore(\n",
    "                    corpus = corpus,\n",
    "                    id2word=common_dictionary,\n",
    "                    num_topics = topics, \n",
    "                    passes = passes,\n",
    "                    iterations = iterations,\n",
    "                    alpha = alpha, \n",
    "                    eta = eta,\n",
    "                    random_state = 42\n",
    "                   )\n",
    "    # Compute Perplexity\n",
    "    print(\"Perplexity: \", lda_model.log_perplexity(df_complaints))  # a measure of how good the model is. lower = better.\n",
    "    # save the trained model\n",
    "    print(\"Saving the model...\")\n",
    "    lda_model.save(\"/runs_Sprint3/LDA/lda_model_\" + str(topics) + \"topics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "lda_model5 = LdaModel.load(\"/runs_Sprint3/LDA/lda_model_\" + 5 +\"topics\")\n",
    "lda_model10 = LdaModel.load(\"/runs_Sprint3/LDA/lda_model_\" + 10 +\"topics\")\n",
    "lda_model15 = LdaModel.load(\"/runs_Sprint3/LDA/lda_model_\" + 15 +\"topics\")\n",
    "lda_model20 = LdaModel.load(\"/runs_Sprint3/LDA/lda_model_\" + 20 +\"topics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
